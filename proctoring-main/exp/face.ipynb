{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-26T07:19:44.330647Z",
     "start_time": "2024-07-26T07:19:43.080322Z"
    }
   },
   "source": [
    "import face_recognition\n",
    "import face_recognition_models"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "face_recognition_models.cnn_face_detector_model_location()",
   "id": "342c021a94fad3c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T07:39:32.419715Z",
     "start_time": "2024-07-26T07:39:32.388465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "image"
   ],
   "id": "5ddc9577676f0fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 91,  33,  29],\n",
       "        [ 86,  28,  24],\n",
       "        [ 82,  24,  20],\n",
       "        ...,\n",
       "        [189, 178, 148],\n",
       "        [194, 183, 153],\n",
       "        [194, 184, 157]],\n",
       "\n",
       "       [[ 91,  33,  29],\n",
       "        [ 86,  28,  24],\n",
       "        [ 83,  25,  21],\n",
       "        ...,\n",
       "        [190, 179, 149],\n",
       "        [195, 184, 154],\n",
       "        [196, 187, 158]],\n",
       "\n",
       "       [[ 91,  33,  29],\n",
       "        [ 86,  28,  24],\n",
       "        [ 83,  25,  21],\n",
       "        ...,\n",
       "        [191, 180, 148],\n",
       "        [197, 186, 154],\n",
       "        [198, 189, 160]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[129,  54,  14],\n",
       "        [128,  53,  14],\n",
       "        [125,  49,  13],\n",
       "        ...,\n",
       "        [155, 124, 103],\n",
       "        [152, 121, 100],\n",
       "        [152, 121, 100]],\n",
       "\n",
       "       [[128,  52,  16],\n",
       "        [127,  51,  15],\n",
       "        [123,  47,  15],\n",
       "        ...,\n",
       "        [166, 134, 109],\n",
       "        [164, 132, 107],\n",
       "        [163, 131, 106]],\n",
       "\n",
       "       [[126,  50,  18],\n",
       "        [128,  52,  20],\n",
       "        [122,  45,  17],\n",
       "        ...,\n",
       "        [169, 139, 113],\n",
       "        [164, 134, 108],\n",
       "        [162, 132, 106]]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T07:40:05.541057Z",
     "start_time": "2024-07-26T07:40:05.494182Z"
    }
   },
   "cell_type": "code",
   "source": "face_locations = face_recognition.face_locations(image, model=\"cnn\")",
   "id": "a49bd961610f61b0",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m face_locations \u001B[38;5;241m=\u001B[39m \u001B[43mface_recognition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mface_locations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcnn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\proctoring-main\\.venv\\lib\\site-packages\\face_recognition\\api.py:119\u001B[0m, in \u001B[0;36mface_locations\u001B[1;34m(img, number_of_times_to_upsample, model)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;124;03mReturns an array of bounding boxes of human faces in a image\u001B[39;00m\n\u001B[0;32m    111\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;124;03m:return: A list of tuples of found face locations in css (top, right, bottom, left) order\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcnn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [_trim_css_to_bounds(_rect_to_css(face\u001B[38;5;241m.\u001B[39mrect), img\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;28;01mfor\u001B[39;00m face \u001B[38;5;129;01min\u001B[39;00m \u001B[43m_raw_face_locations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_times_to_upsample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcnn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m]\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [_trim_css_to_bounds(_rect_to_css(face), img\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;28;01mfor\u001B[39;00m face \u001B[38;5;129;01min\u001B[39;00m _raw_face_locations(img, number_of_times_to_upsample, model)]\n",
      "File \u001B[1;32m~\\Desktop\\proctoring-main\\.venv\\lib\\site-packages\\face_recognition\\api.py:103\u001B[0m, in \u001B[0;36m_raw_face_locations\u001B[1;34m(img, number_of_times_to_upsample, model)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;124;03mReturns an array of bounding boxes of human faces in a image\u001B[39;00m\n\u001B[0;32m     95\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;124;03m:return: A list of dlib 'rect' objects of found face locations\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcnn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcnn_face_detector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_times_to_upsample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m face_detector(img, number_of_times_to_upsample)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4a0c9a960d8119ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
